{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7e6387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group6_project.py\n",
    "# Satchytan Karalasingham, Romil Dhagat\n",
    "#\n",
    "'''\n",
    "A terminal-based application for computing and printing statistics based on given input.\n",
    "You must include the main listed below. You may add your own additional classes, functions, variables, etc. \n",
    "You may import any modules from the standard Python library.\n",
    "Remember to include docstrings and comments.\n",
    "'''\n",
    "\n",
    "def main():\n",
    "\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Stage 2: DataFrame Creation\n",
    "    \n",
    "    # Import the Excel data into a Pandas DataFrames.\n",
    "    df_goog = pd.read_excel('Google_Data.xlsx')\n",
    "    df_meta = pd.read_excel('META_Data.xlsx')\n",
    "    df_msft = pd.read_excel('Microsoft_data.xlsx')\n",
    "    df_tsla = pd.read_excel('Tesla_Data.xlsx')\n",
    "\n",
    "    # Merge the four DataFrames based on a common column\n",
    "    # merged_df = pd.merge(df_goog, df_meta, on='Date')\n",
    "    # merged_df = pd.merge(merged_df, df_msft, on='Date')\n",
    "    # merged_df = pd.merge(merged_df, df_tsla, on='Date')\n",
    "\n",
    "    # Remove any duplicated columns resulting from the merge\n",
    "    # merged_df = merged_df.loc[:, ~merged_df.columns.duplicated()]\n",
    "\n",
    "    # Create a hierarchical index using two levels (row or column)\n",
    "    # merged_df.set_index(['Date'] , inplace=True)                      # RD : Removed ['Stock']\n",
    "\n",
    "    # Sort the data based on the index\n",
    "    # merged_df.sort_index(inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "    print(\"ENSF 592 Group 6 Project\")\n",
    "\n",
    "\n",
    "\n",
    "    # Stage 3: User Input\n",
    "    # Prompt the user to enter a stock ticker.\n",
    "    while True:\n",
    "        print(\"There are four stocks to compare: GOOG, MSFT, TSLA, META\")\n",
    "        ticker1 = input(\"Please enter a stock ticker: \").upper()\n",
    "        ticker2 = input(\"Please enter another stock ticker: \").upper()\n",
    "\n",
    "        try:\n",
    "\n",
    "            if (ticker1 == 'GOOG' and ticker2 == 'META') or (ticker1 == 'META' and ticker2 == 'GOOG'):                       \n",
    "                # Merge the selected DataFrames based on a common column\n",
    "                merged_df = pd.merge(df_goog, df_meta, on='Date')\n",
    "                # Remove any duplicated columns resulting from the merge\n",
    "                merged_df = merged_df.loc[:, ~merged_df.columns.duplicated()] \n",
    "                # Create a hierarchical index using two levels (row or column)\n",
    "                merged_df.set_index(['Date'] , inplace=True) \n",
    "                # Sort the data based on the index\n",
    "                merged_df.sort_index(inplace=True)\n",
    "            elif (ticker1 == 'GOOG' and ticker2 == 'MSFT') or (ticker1 == 'MSFT' and ticker2 == 'GOOG'):\n",
    "                # Merge the selected DataFrames based on a common column               \n",
    "                merged_df = pd.merge(df_goog, df_msft, on='Date')   \n",
    "                # Remove any duplicated columns resulting from the merge\n",
    "                merged_df = merged_df.loc[:, ~merged_df.columns.duplicated()] \n",
    "                # Create a hierarchical index using two levels (row or column)\n",
    "                merged_df.set_index(['Date'] , inplace=True) \n",
    "                # Sort the data based on the index\n",
    "                merged_df.sort_index(inplace=True)                                                      \n",
    "            elif (ticker1 == 'GOOG' and ticker2 == 'TSLA') or (ticker1 == 'TSLA' and ticker2 == 'GOOG'):  \n",
    "                # Merge the selected DataFrames based on a common column                   \n",
    "                merged_df = pd.merge(df_goog, df_tsla, on='Date') \n",
    "                # Remove any duplicated columns resulting from the merge\n",
    "                merged_df = merged_df.loc[:, ~merged_df.columns.duplicated()] \n",
    "                # Create a hierarchical index using two levels (row or column)\n",
    "                merged_df.set_index(['Date'] , inplace=True)  \n",
    "                # Sort the data based on the index\n",
    "                merged_df.sort_index(inplace=True)                                                       \n",
    "            elif (ticker1 == 'MSFT' and ticker2 == 'META') or (ticker1 == 'META' and ticker2 == 'MSFT'):\n",
    "                # Merge the selected DataFrames based on a common column\n",
    "                merged_df = pd.merge(df_msft, df_meta, on='Date')\n",
    "                # Remove any duplicated columns resulting from the merge\n",
    "                merged_df = merged_df.loc[:, ~merged_df.columns.duplicated()] \n",
    "                # Create a hierarchical index using two levels (row or column)\n",
    "                merged_df.set_index(['Date'] , inplace=True)\n",
    "                # Sort the data based on the index\n",
    "                merged_df.sort_index(inplace=True)\n",
    "            elif (ticker1 == 'MSFT' and ticker2 == 'TSLA') or (ticker1 == 'TSLA' and ticker2 == 'MSFT'):\n",
    "                # Merge the selected DataFrames based on a common column\n",
    "                merged_df = pd.merge(df_tsla, df_msft, on='Date')\n",
    "                # Remove any duplicated columns resulting from the merge\n",
    "                merged_df = merged_df.loc[:, ~merged_df.columns.duplicated()] \n",
    "                # Create a hierarchical index using two levels (row or column)\n",
    "                merged_df.set_index(['Date'] , inplace=True)\n",
    "                # Sort the data based on the index\n",
    "                merged_df.sort_index(inplace=True)\n",
    "            elif (ticker1 == 'META' and ticker2 == 'TSLA') or (ticker1 == 'TSLA' and ticker2 == 'META'):\n",
    "                # Merge the selected DataFrames based on a common column\n",
    "                merged_df = pd.merge(df_meta, df_tsla, on='Date')\n",
    "                # Remove any duplicated columns resulting from the merge\n",
    "                merged_df = merged_df.loc[:, ~merged_df.columns.duplicated()] \n",
    "                # Create a hierarchical index using two levels (row or column)\n",
    "                merged_df.set_index(['Date'] , inplace=True)\n",
    "                # Sort the data based on the index\n",
    "                merged_df.sort_index(inplace=True)\n",
    "            else:\n",
    "                print('first')\n",
    "                raise KeyError\n",
    "\n",
    "            # Stage 4: Analysis and Calculations\n",
    "            # filtered_df = merged_df.loc[(slice(None), [ticker1, ticker2]), :]\n",
    "\n",
    "            if merged_df.empty:\n",
    "                print(\"second\")\n",
    "                raise KeyError\n",
    "            else:\n",
    "                # Display the filtered data\n",
    "                print(\"\\nFiltered Data:\")\n",
    "                print(merged_df)\n",
    "\n",
    "                # Aggregate statistics using describe method\n",
    "                print(\"\\nAggregate Statistics:\")\n",
    "                print(merged_df.describe())\n",
    "                \n",
    "                # Add two columns to the dataset\n",
    "                merged_df['Daily Return_x'] = merged_df['Close_x'] - merged_df['Open_x']\n",
    "                merged_df['Daily Return_y'] = merged_df['Close_y'] - merged_df['Open_y']\n",
    "                merged_df['Volume in Millions'] = merged_df['Volume'] / 1000000\n",
    "\n",
    "                print(merged_df['Daily Return_x'])\n",
    "\n",
    "                # Perform aggregation computation for a subset of data\n",
    "                mean_return = merged_df.loc[(slice(None), [ticker1, ticker2]), 'Daily Return'].mean()\n",
    "                print(f\"\\nMean Daily Return for {ticker1} and {ticker2}: {mean_return:.2f}\")\n",
    "\n",
    "                # Perform masking operation\n",
    "                high_volume_df = merged_df[merged_df['Volume'] > 50000000]\n",
    "                print(\"\\nData with Volume > 50,000,000:\")\n",
    "                print(high_volume_df)\n",
    "\n",
    "                # Perform groupby operation\n",
    "                grouped_df = merged_df.groupby('Stock').mean()\n",
    "                print(\"\\nGrouped Data (Mean values per stock):\")\n",
    "                print(grouped_df)\n",
    "\n",
    "                # Create and print a pivot table\n",
    "                pivot_table = pd.pivot_table(merged_df, values='Close', index='Date', columns='Stock')\n",
    "                print(\"\\nPivot Table:\")\n",
    "                print(pivot_table)\n",
    "\n",
    "                # Stage 5: Export and Matplotlib\n",
    "                # Export merged DataFrame to Excel file\n",
    "                merged_df.to_excel('merged_data.xlsx')\n",
    "\n",
    "                # Create a plot using Matplotlib\n",
    "                merged_df.plot(x='Date', y='Close', marker='o')\n",
    "                plt.title(f\"Stock Prices - {ticker1} and {ticker2}\")\n",
    "                plt.xlabel(\"Date\")\n",
    "                plt.ylabel(\"Closing Price\")\n",
    "                plt.grid(True)\n",
    "                plt.savefig('stock_prices.png')\n",
    "                plt.show()\n",
    "\n",
    "        except KeyError:\n",
    "            print(\"Invalid stock ticker. Please try again.\\n\")\n",
    "            continue\n",
    "\n",
    "        another_search = input(\"\\nDo you want to search for another pair of tickers? (y/n): \")\n",
    "        if another_search.lower() != 'n':\n",
    "            break\n",
    "\n",
    "    print(\"\\nProgram finished.\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "655768f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Stage 2: DataFrame Creation\n",
    "\n",
    "# Import the Excel data into a Pandas DataFrames.\n",
    "df_goog = pd.read_excel('Google_Data.xlsx')\n",
    "df_meta = pd.read_excel('META_Data.xlsx')\n",
    "df_msft = pd.read_excel('Microsoft_data.xlsx')\n",
    "df_tsla = pd.read_excel('Tesla_Data.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "551e534c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the selected DataFrames based on a common column\n",
    "merged_df = pd.merge(df_goog, df_meta, on='Date')\n",
    "# Remove any duplicated columns resulting from the merge\n",
    "merged_df = merged_df.loc[:, ~merged_df.columns.duplicated()] \n",
    "# Create a hierarchical index using two levels (row or column)\n",
    "merged_df.set_index(['Date'] , inplace=True) \n",
    "# Sort the data based on the index\n",
    "merged_df.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "affefd84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open_x</th>\n",
       "      <th>High_x</th>\n",
       "      <th>Low_x</th>\n",
       "      <th>Close_x</th>\n",
       "      <th>Volume_x</th>\n",
       "      <th>Dividends_x</th>\n",
       "      <th>Stock Splits_x</th>\n",
       "      <th>Open_y</th>\n",
       "      <th>High_y</th>\n",
       "      <th>Low_y</th>\n",
       "      <th>Close_y</th>\n",
       "      <th>Volume_y</th>\n",
       "      <th>Dividends_y</th>\n",
       "      <th>Stock Splits_y</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-06-09</th>\n",
       "      <td>116.341499</td>\n",
       "      <td>118.349998</td>\n",
       "      <td>114.866997</td>\n",
       "      <td>114.917999</td>\n",
       "      <td>23142000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>194.279999</td>\n",
       "      <td>199.449997</td>\n",
       "      <td>183.679993</td>\n",
       "      <td>184.000000</td>\n",
       "      <td>23501600</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-10</th>\n",
       "      <td>112.781250</td>\n",
       "      <td>113.497002</td>\n",
       "      <td>110.861000</td>\n",
       "      <td>111.427498</td>\n",
       "      <td>31324000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>183.039993</td>\n",
       "      <td>183.100006</td>\n",
       "      <td>175.020004</td>\n",
       "      <td>175.570007</td>\n",
       "      <td>27450800</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-13</th>\n",
       "      <td>107.445999</td>\n",
       "      <td>109.218498</td>\n",
       "      <td>106.588051</td>\n",
       "      <td>106.876503</td>\n",
       "      <td>36756000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>170.589996</td>\n",
       "      <td>172.580002</td>\n",
       "      <td>164.029999</td>\n",
       "      <td>164.259995</td>\n",
       "      <td>31749300</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-14</th>\n",
       "      <td>106.889999</td>\n",
       "      <td>108.457497</td>\n",
       "      <td>106.351997</td>\n",
       "      <td>107.194000</td>\n",
       "      <td>25480000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>166.029999</td>\n",
       "      <td>166.750000</td>\n",
       "      <td>161.360001</td>\n",
       "      <td>163.729996</td>\n",
       "      <td>27244300</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-15</th>\n",
       "      <td>108.899498</td>\n",
       "      <td>112.063004</td>\n",
       "      <td>108.118752</td>\n",
       "      <td>110.390503</td>\n",
       "      <td>33192000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>167.199997</td>\n",
       "      <td>172.160004</td>\n",
       "      <td>163.979996</td>\n",
       "      <td>169.350006</td>\n",
       "      <td>30008300</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-02</th>\n",
       "      <td>124.489998</td>\n",
       "      <td>126.745003</td>\n",
       "      <td>124.349998</td>\n",
       "      <td>125.230003</td>\n",
       "      <td>19362400</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>272.660004</td>\n",
       "      <td>275.350006</td>\n",
       "      <td>271.119995</td>\n",
       "      <td>272.609985</td>\n",
       "      <td>19405300</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-05</th>\n",
       "      <td>124.610001</td>\n",
       "      <td>127.989998</td>\n",
       "      <td>124.379997</td>\n",
       "      <td>126.629997</td>\n",
       "      <td>22672500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>270.299988</td>\n",
       "      <td>275.570007</td>\n",
       "      <td>269.559998</td>\n",
       "      <td>271.390015</td>\n",
       "      <td>20742900</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-06</th>\n",
       "      <td>126.599998</td>\n",
       "      <td>128.880005</td>\n",
       "      <td>125.970001</td>\n",
       "      <td>127.910004</td>\n",
       "      <td>19450100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>270.140015</td>\n",
       "      <td>276.570007</td>\n",
       "      <td>269.690002</td>\n",
       "      <td>271.119995</td>\n",
       "      <td>19419000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-07</th>\n",
       "      <td>127.574997</td>\n",
       "      <td>129.550003</td>\n",
       "      <td>122.629997</td>\n",
       "      <td>122.940002</td>\n",
       "      <td>34179300</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>271.670013</td>\n",
       "      <td>274.250000</td>\n",
       "      <td>262.799988</td>\n",
       "      <td>263.600006</td>\n",
       "      <td>26163600</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-08</th>\n",
       "      <td>122.584999</td>\n",
       "      <td>123.730003</td>\n",
       "      <td>122.010002</td>\n",
       "      <td>122.669998</td>\n",
       "      <td>23954447</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>260.619995</td>\n",
       "      <td>267.640015</td>\n",
       "      <td>258.899994</td>\n",
       "      <td>264.579987</td>\n",
       "      <td>20841315</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>251 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Open_x      High_x       Low_x     Close_x  Volume_x  \\\n",
       "Date                                                                   \n",
       "2022-06-09  116.341499  118.349998  114.866997  114.917999  23142000   \n",
       "2022-06-10  112.781250  113.497002  110.861000  111.427498  31324000   \n",
       "2022-06-13  107.445999  109.218498  106.588051  106.876503  36756000   \n",
       "2022-06-14  106.889999  108.457497  106.351997  107.194000  25480000   \n",
       "2022-06-15  108.899498  112.063004  108.118752  110.390503  33192000   \n",
       "...                ...         ...         ...         ...       ...   \n",
       "2023-06-02  124.489998  126.745003  124.349998  125.230003  19362400   \n",
       "2023-06-05  124.610001  127.989998  124.379997  126.629997  22672500   \n",
       "2023-06-06  126.599998  128.880005  125.970001  127.910004  19450100   \n",
       "2023-06-07  127.574997  129.550003  122.629997  122.940002  34179300   \n",
       "2023-06-08  122.584999  123.730003  122.010002  122.669998  23954447   \n",
       "\n",
       "            Dividends_x  Stock Splits_x      Open_y      High_y       Low_y  \\\n",
       "Date                                                                          \n",
       "2022-06-09            0               0  194.279999  199.449997  183.679993   \n",
       "2022-06-10            0               0  183.039993  183.100006  175.020004   \n",
       "2022-06-13            0               0  170.589996  172.580002  164.029999   \n",
       "2022-06-14            0               0  166.029999  166.750000  161.360001   \n",
       "2022-06-15            0               0  167.199997  172.160004  163.979996   \n",
       "...                 ...             ...         ...         ...         ...   \n",
       "2023-06-02            0               0  272.660004  275.350006  271.119995   \n",
       "2023-06-05            0               0  270.299988  275.570007  269.559998   \n",
       "2023-06-06            0               0  270.140015  276.570007  269.690002   \n",
       "2023-06-07            0               0  271.670013  274.250000  262.799988   \n",
       "2023-06-08            0               0  260.619995  267.640015  258.899994   \n",
       "\n",
       "               Close_y  Volume_y  Dividends_y  Stock Splits_y  \n",
       "Date                                                           \n",
       "2022-06-09  184.000000  23501600            0               0  \n",
       "2022-06-10  175.570007  27450800            0               0  \n",
       "2022-06-13  164.259995  31749300            0               0  \n",
       "2022-06-14  163.729996  27244300            0               0  \n",
       "2022-06-15  169.350006  30008300            0               0  \n",
       "...                ...       ...          ...             ...  \n",
       "2023-06-02  272.609985  19405300            0               0  \n",
       "2023-06-05  271.390015  20742900            0               0  \n",
       "2023-06-06  271.119995  19419000            0               0  \n",
       "2023-06-07  263.600006  26163600            0               0  \n",
       "2023-06-08  264.579987  20841315            0               0  \n",
       "\n",
       "[251 rows x 14 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c991af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['Daily Return_x'] = merged_df['Close_x'] - merged_df['Open_x']\n",
    "merged_df['Daily Return_y'] = merged_df['Close_y'] - merged_df['Open_y']\n",
    "merged_df['Volume in Millions'] = merged_df['Volume_x'] / 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05931578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date\n",
       "2022-06-09   -1.423500\n",
       "2022-06-10   -1.353752\n",
       "2022-06-13   -0.569496\n",
       "2022-06-14    0.304001\n",
       "2022-06-15    1.491005\n",
       "                ...   \n",
       "2023-06-02    0.740005\n",
       "2023-06-05    2.019997\n",
       "2023-06-06    1.310005\n",
       "2023-06-07   -4.634995\n",
       "2023-06-08    0.084999\n",
       "Name: Daily Return_x, Length: 251, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df['Daily Return_x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ab9dc53",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'goog' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13052\\2806236903.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Perform aggregation computation for a subset of data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmean_return\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmerged_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mslice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mgoog\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeta\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Daily Return'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m#print(f\"\\nMean Daily Return for Google and meta: {mean_return:.2f}\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'goog' is not defined"
     ]
    }
   ],
   "source": [
    "# Perform aggregation computation for a subset of data\n",
    "mean_return = merged_df.loc[(slice(None), [goog, meta]), 'Daily Return'].mean()\n",
    "#print(f\"\\nMean Daily Return for Google and meta: {mean_return:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
