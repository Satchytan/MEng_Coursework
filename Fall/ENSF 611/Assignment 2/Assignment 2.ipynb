{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92778525",
   "metadata": {},
   "source": [
    "# Assignment 2: Linear Models and Validation Metrics (30 marks total)\n",
    "### Due: October 10 at 11:59pm\n",
    "\n",
    "### Name: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce31b39a",
   "metadata": {},
   "source": [
    "### In this assignment, you will need to write code that uses linear models to perform classification and regression tasks. You will also be asked to describe the process by which you came up with the code. More details can be found below. Please cite any websites or AI tools that you used to help you with this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c6de86",
   "metadata": {},
   "source": [
    "## Part 1: Classification (14.5 marks total)\n",
    "\n",
    "You have been asked to develop code that can help the user determine if the email they have received is spam or not. Following the machine learning workflow described in class, write the relevant code in each of the steps below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3c6fc8",
   "metadata": {},
   "source": [
    "### Step 0: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33f86925",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yellowbrick as yb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9d33a8",
   "metadata": {},
   "source": [
    "### Step 1: Data Input (1 mark)\n",
    "\n",
    "The data used for this task can be downloaded using the yellowbrick library: \n",
    "https://www.scikit-yb.org/en/latest/api/datasets/spam.html\n",
    "\n",
    "Use the yellowbrick function `load_spam()` to load the spam dataset into the feature matrix `X` and target vector `y`.\n",
    "\n",
    "Print the size and type of `X` and `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "33583c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X: (4600, 57)\n",
      "Type of X: <class 'pandas.core.frame.DataFrame'>\n",
      "word_freq_make                float64\n",
      "word_freq_address             float64\n",
      "word_freq_all                 float64\n",
      "word_freq_3d                  float64\n",
      "word_freq_our                 float64\n",
      "word_freq_over                float64\n",
      "word_freq_remove              float64\n",
      "word_freq_internet            float64\n",
      "word_freq_order               float64\n",
      "word_freq_mail                float64\n",
      "word_freq_receive             float64\n",
      "word_freq_will                float64\n",
      "word_freq_people              float64\n",
      "word_freq_report              float64\n",
      "word_freq_addresses           float64\n",
      "word_freq_free                float64\n",
      "word_freq_business            float64\n",
      "word_freq_email               float64\n",
      "word_freq_you                 float64\n",
      "word_freq_credit              float64\n",
      "word_freq_your                float64\n",
      "word_freq_font                float64\n",
      "word_freq_000                 float64\n",
      "word_freq_money               float64\n",
      "word_freq_hp                  float64\n",
      "word_freq_hpl                 float64\n",
      "word_freq_george              float64\n",
      "word_freq_650                 float64\n",
      "word_freq_lab                 float64\n",
      "word_freq_labs                float64\n",
      "word_freq_telnet              float64\n",
      "word_freq_857                 float64\n",
      "word_freq_data                float64\n",
      "word_freq_415                 float64\n",
      "word_freq_85                  float64\n",
      "word_freq_technology          float64\n",
      "word_freq_1999                float64\n",
      "word_freq_parts               float64\n",
      "word_freq_pm                  float64\n",
      "word_freq_direct              float64\n",
      "word_freq_cs                  float64\n",
      "word_freq_meeting             float64\n",
      "word_freq_original            float64\n",
      "word_freq_project             float64\n",
      "word_freq_re                  float64\n",
      "word_freq_edu                 float64\n",
      "word_freq_table               float64\n",
      "word_freq_conference          float64\n",
      "char_freq_;                   float64\n",
      "char_freq_(                   float64\n",
      "char_freq_[                   float64\n",
      "char_freq_!                   float64\n",
      "char_freq_$                   float64\n",
      "char_freq_#                   float64\n",
      "capital_run_length_average    float64\n",
      "capital_run_length_longest      int64\n",
      "capital_run_length_total        int64\n",
      "dtype: object\n",
      "Size of y: (4600,)\n",
      "Type of y: <class 'pandas.core.series.Series'>\n",
      "int64\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Import spam dataset from yellowbrick library\n",
    "from yellowbrick.datasets import load_spam\n",
    "# TO DO: Print size and type of X and y\n",
    "X, y = load_spam()\n",
    "\n",
    "print(\"Size of X:\", X.shape)\n",
    "print(\"Type of X:\", type(X))\n",
    "print(X.dtypes)\n",
    "print(\"Size of y:\", y.shape)\n",
    "print(\"Type of y:\", type(y))\n",
    "print(y.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156db208",
   "metadata": {},
   "source": [
    "### Step 2: Data Processing (1.5 marks)\n",
    "\n",
    "Check to see if there are any missing values in the dataset. If necessary, select an appropriate method to fill-in the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4e7204f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      " word_freq_make                0\n",
      "word_freq_address             0\n",
      "word_freq_all                 0\n",
      "word_freq_3d                  0\n",
      "word_freq_our                 0\n",
      "word_freq_over                0\n",
      "word_freq_remove              0\n",
      "word_freq_internet            0\n",
      "word_freq_order               0\n",
      "word_freq_mail                0\n",
      "word_freq_receive             0\n",
      "word_freq_will                0\n",
      "word_freq_people              0\n",
      "word_freq_report              0\n",
      "word_freq_addresses           0\n",
      "word_freq_free                0\n",
      "word_freq_business            0\n",
      "word_freq_email               0\n",
      "word_freq_you                 0\n",
      "word_freq_credit              0\n",
      "word_freq_your                0\n",
      "word_freq_font                0\n",
      "word_freq_000                 0\n",
      "word_freq_money               0\n",
      "word_freq_hp                  0\n",
      "word_freq_hpl                 0\n",
      "word_freq_george              0\n",
      "word_freq_650                 0\n",
      "word_freq_lab                 0\n",
      "word_freq_labs                0\n",
      "word_freq_telnet              0\n",
      "word_freq_857                 0\n",
      "word_freq_data                0\n",
      "word_freq_415                 0\n",
      "word_freq_85                  0\n",
      "word_freq_technology          0\n",
      "word_freq_1999                0\n",
      "word_freq_parts               0\n",
      "word_freq_pm                  0\n",
      "word_freq_direct              0\n",
      "word_freq_cs                  0\n",
      "word_freq_meeting             0\n",
      "word_freq_original            0\n",
      "word_freq_project             0\n",
      "word_freq_re                  0\n",
      "word_freq_edu                 0\n",
      "word_freq_table               0\n",
      "word_freq_conference          0\n",
      "char_freq_;                   0\n",
      "char_freq_(                   0\n",
      "char_freq_[                   0\n",
      "char_freq_!                   0\n",
      "char_freq_$                   0\n",
      "char_freq_#                   0\n",
      "capital_run_length_average    0\n",
      "capital_run_length_longest    0\n",
      "capital_run_length_total      0\n",
      "dtype: int64\n",
      "0\n",
      " 0\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Check if there are any missing values and fill them in if necessary\n",
    "missing_values_X = X.isnull().sum().sum()\n",
    "print(missing_values_X)\n",
    "missing_x_values = X.isna().sum()\n",
    "print(\"\", missing_x_values)\n",
    "\n",
    "\n",
    "missing_values_y = y.isnull().sum().sum()\n",
    "print(missing_values_y)\n",
    "missing_y_values = y.isna().sum()\n",
    "print(\"\", missing_y_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922a70ca",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a489285a",
   "metadata": {},
   "source": [
    "For this task, we want to test if the linear model would still work if we used less data. Use the `train_test_split` function from sklearn to create a new feature matrix named `X_small` and a new target vector named `y_small` that contain **5%** of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f9bc4a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X_small: (230, 57)\n",
      "Type of X_small: <class 'pandas.core.frame.DataFrame'>\n",
      "Size of y_small: (230,)\n",
      "Type of y_small: <class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Create X_small and y_small \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets, retaining 5% of the data for testing\n",
    "X_small, _, y_small, _ = train_test_split(X, y, test_size=0.95, random_state=42)\n",
    "\n",
    "# Print the size and type of X_small and y_small\n",
    "print(\"Size of X_small:\", X_small.shape)\n",
    "print(\"Type of X_small:\", type(X_small))\n",
    "print(\"Size of y_small:\", y_small.shape)\n",
    "print(\"Type of y_small:\", type(y_small))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e6c46f",
   "metadata": {},
   "source": [
    "### Step 3: Implement Machine Learning Model\n",
    "\n",
    "1. Import `LogisticRegression` from sklearn\n",
    "2. Instantiate model `LogisticRegression(max_iter=2000)`.\n",
    "3. Implement the machine learning model with three different datasets: \n",
    "    - `X` and `y`\n",
    "    - Only first two columns of `X` and `y`\n",
    "    - `X_small` and `y_small`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89f3d84",
   "metadata": {},
   "source": [
    "### Step 4: Validate Model\n",
    "\n",
    "Calculate the training and validation accuracy for the three different tests implemented in Step 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352106a3",
   "metadata": {},
   "source": [
    "### Step 5: Visualize Results (4 marks)\n",
    "\n",
    "1. Create a pandas DataFrame `results` with columns: Data size, training accuracy, validation accuracy\n",
    "2. Add the data size, training and validation accuracy for each dataset to the `results` DataFrame\n",
    "3. Print `results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "be4b5c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Data Size  Data Shape  Training Accuracy  Validation Accuracy\n",
      "0            Full Dataset  (3680, 57)           0.932391             0.917391\n",
      "1  First Two Columns of X   (3680, 2)           0.616304             0.590217\n",
      "2                 X_small   (184, 57)           0.960870             0.934783\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE FOR STEPS 3-5\n",
    "# STEP 3\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create a validation dataset for each model\n",
    "X_train_original, X_val_original, y_train_original, y_val_original = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train_first_two_columns, X_val_first_two_columns, y_train_first_two_columns, y_val_first_two_columns = train_test_split(X.iloc[:, :2], y, test_size=0.2, random_state=42)\n",
    "X_train_small, X_val_small, y_train_small, y_val_small = train_test_split(X_small, y_small, test_size=0.2, random_state=42)\n",
    "\n",
    "# Instantiate a new Logistic Regression model with a maximum of 2000 iterations for each dataset\n",
    "model_original = LogisticRegression(max_iter=2000)\n",
    "model_first_two_columns = LogisticRegression(max_iter=2000)\n",
    "model_small = LogisticRegression(max_iter=2000)\n",
    "\n",
    "# Fit each model with its respective dataset\n",
    "model_original.fit(X.values, y.values)\n",
    "model_first_two_columns.fit(X.iloc[:, :2].values, y.values)\n",
    "model_small.fit(X_small.values, y_small.values)\n",
    "\n",
    "# STEP 4\n",
    "\n",
    "# Calculate and print the training accuracy for each model\n",
    "train_accuracy_original = model_original.score(X.values, y.values)\n",
    "train_accuracy_first_two_columns = model_first_two_columns.score(X.iloc[:, :2].values, y.values)\n",
    "train_accuracy_small = model_small.score(X_small.values, y_small.values)\n",
    "\n",
    "# Calculate and print validation accuracy for each model\n",
    "val_accuracy_original = model_original.score(X_val_original.values, y_val_original.values)\n",
    "val_accuracy_first_two_columns = model_first_two_columns.score(X_val_first_two_columns.values, y_val_first_two_columns.values)\n",
    "val_accuracy_small = model_small.score(X_val_small.values, y_val_small.values)\n",
    "\n",
    "# STEP 5\n",
    "\n",
    "# Create a list to store the results for each dataset\n",
    "data_sizes = [\"Full Dataset\", \"First Two Columns of X\", \"X_small\"]\n",
    "data_shapes = [X_train_original.shape, X_train_first_two_columns.shape, X_train_small.shape]\n",
    "training_accuracies = [train_accuracy_original, train_accuracy_first_two_columns, train_accuracy_small]\n",
    "validation_accuracies = [val_accuracy_original, val_accuracy_first_two_columns, val_accuracy_small]\n",
    "\n",
    "# Create the results DataFrame\n",
    "results = pd.DataFrame({\n",
    "    \"Data Size\": data_sizes,\n",
    "    \"Data Shape\": data_shapes,\n",
    "    \"Training Accuracy\": training_accuracies,\n",
    "    \"Validation Accuracy\": validation_accuracies\n",
    "})\n",
    "\n",
    "# Print the results DataFrame\n",
    "print(results)\n",
    "\n",
    "\n",
    "# Note: for any random state parameters, you can use random_state = 0\n",
    "# HINT: USING A LOOP TO STORE THE DATA IN YOUR RESULTS DATAFRAME WILL BE MORE EFFICIENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4427d4f",
   "metadata": {},
   "source": [
    "### Questions (4 marks)\n",
    "1. How do the training and validation accuracy change depending on the amount of data used? Explain with values.\n",
    "2. In this case, what do a false positive and a false negative represent? Which one is worse?\n",
    "\n",
    "*YOUR ANSWERS HERE*\n",
    "\n",
    "1. Training and Validation Accuracy vs. Amount of Data:\n",
    "\n",
    "Full Dataset: The \"Full Dataset\" exhibits the highest training and validation accuracy. This indicates that the model has been trained on a large amount of data, and it performs well not only on the training data but also on unseen validation data. The training and validation accuracy values for the \"Full Dataset\" are 0.932391 and 0.917391, respectively.\n",
    "\n",
    "First Two Columns of X: The \"First Two Columns of X\" dataset has lower training and validation accuracy compared to the \"Full Dataset.\" This suggests that when you use a subset of features (only the first two columns of X), the model's predictive power is reduced. The training and validation accuracy values for this dataset are 0.616304 and 0.590217, respectively.\n",
    "\n",
    "X_small: The \"X_small\" dataset achieves high training and validation accuracy, even though it contains a smaller amount of data. This indicates that the model can generalize well from the limited data available in this dataset. The training and validation accuracy values for this dataset are 0.960870 and 0.934783, respectively.\n",
    "\n",
    "Explanation: In general, as you increase the amount of training data, both training and validation accuracy tend to improve. This is because a larger training dataset provides the model with more information to learn from, resulting in better performance. However, it's important to note that increasing the amount of data does not guarantee improvement, and there can be diminishing returns beyond a certain point. The specific behavior may vary depending on the complexity of the model and the quality of the data.\n",
    "\n",
    "\n",
    "\n",
    "2. False Positive and False Negative:\n",
    "\n",
    "In the context of a binary classification problem (e.g., spam classification), a false positive occurs when the model incorrectly predicts a positive (spam) class when the actual class is negative (not spam). In other words, it's a situation where the model incorrectly identifies something as spam when it's not.\n",
    "\n",
    "A false negative occurs when the model incorrectly predicts a negative (not spam) class when the actual class is positive (spam). In this case, the model fails to identify actual spam.\n",
    "\n",
    "Which One Is Worse?:\n",
    "\n",
    "False Positives: False positives can be annoying or inconvenient for users because legitimate emails may be classified as spam. However, they generally have fewer severe consequences compared to false negatives.\n",
    "\n",
    "False Negatives: False negatives can be more problematic as they allow actual spam to reach the inbox, potentially causing users to miss important emails or exposing them to potentially harmful content.\n",
    "\n",
    "In many spam classification scenarios, minimizing false negatives (ensuring that spam is accurately detected) is often considered more critical. However, the balance between false positives and false negatives can be adjusted based on user preferences and the specific goals of the application.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7559517a",
   "metadata": {},
   "source": [
    "### Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "1. Where did you source your code?\n",
    "1. In what order did you complete the steps?\n",
    "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fe687f",
   "metadata": {},
   "source": [
    "*DESCRIBE YOUR PROCESS HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4c78a8",
   "metadata": {},
   "source": [
    "## Part 2: Regression (10.5 marks total)\n",
    "\n",
    "For this section, we will be evaluating concrete compressive strength of different concrete samples, based on age and ingredients. You will need to repeat the steps 1-4 from Part 1 for this analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ba83c5",
   "metadata": {},
   "source": [
    "### Step 1: Data Input (1 mark)\n",
    "\n",
    "The data used for this task can be downloaded using the yellowbrick library: \n",
    "https://www.scikit-yb.org/en/latest/api/datasets/concrete.html\n",
    "\n",
    "Use the yellowbrick function `load_concrete()` to load the spam dataset into the feature matrix `X` and target vector `y`.\n",
    "\n",
    "Print the size and type of `X` and `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6ff2e34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X: (1030, 8)\n",
      "Type of X: <class 'pandas.core.frame.DataFrame'>\n",
      "Size of y: (1030,)\n",
      "Type of y: <class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Import spam dataset from yellowbrick library\n",
    "from yellowbrick.datasets import load_concrete\n",
    "# TO DO: Print size and type of X and y\n",
    "X, y = load_concrete()\n",
    "\n",
    "print(\"Size of X:\", X.shape)\n",
    "print(\"Type of X:\", type(X))\n",
    "print(\"Size of y:\", y.shape)\n",
    "print(\"Type of y:\", type(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5294cfa",
   "metadata": {},
   "source": [
    "### Step 2: Data Processing (0.5 marks)\n",
    "\n",
    "Check to see if there are any missing values in the dataset. If necessary, select an appropriate method to fill-in the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "693c5fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Check if there are any missing values and fill them in if necessary\n",
    "missing_values_X = X.isnull().sum().sum()\n",
    "print(missing_values_X)\n",
    "\n",
    "missing_values_y = y.isnull().sum().sum()\n",
    "print(missing_values_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc60489",
   "metadata": {},
   "source": [
    "### Step 3: Implement Machine Learning Model (1 mark)\n",
    "\n",
    "1. Import `LinearRegression` from sklearn\n",
    "2. Instantiate model `LogisticRegression(max_iter=2000)`.\n",
    "3. Implement the machine learning model with `X` and `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b5041945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "# Note: for any random state parameters, you can use random_state = 0\n",
    "\n",
    "# TO DO: ADD YOUR CODE HERE FOR STEPS 3-5\n",
    "# STEP 3\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Create a validation dataset for each model\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Instantiate a new Linear Regression model with a maximum of 2000 iterations for each dataset\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit each model with its respective dataset\n",
    "model.fit(X_train.values, y_train.values)\n",
    "\n",
    "# Fit the model to the training data\n",
    "y_train_pred = model.predict(X_train.values)\n",
    "y_val_pred = model.predict(X_val.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de28482",
   "metadata": {},
   "source": [
    "### Step 4: Validate Model (1 mark)\n",
    "\n",
    "Calculate the training and validation accuracy using mean squared error and R2 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "970c038b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Mean Squared Error:  0.10334473427036059\n",
      "Training R-squared:  0.5641265095933534\n",
      "Validation Mean Squared Error:  0.11476906473081404\n",
      "Validation R-squared:  0.5300409463562602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/satchy/anaconda3/envs/ensf-ml/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/Users/satchy/anaconda3/envs/ensf-ml/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/Users/satchy/anaconda3/envs/ensf-ml/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/Users/satchy/anaconda3/envs/ensf-ml/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/Users/satchy/anaconda3/envs/ensf-ml/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/Users/satchy/anaconda3/envs/ensf-ml/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/Users/satchy/anaconda3/envs/ensf-ml/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/Users/satchy/anaconda3/envs/ensf-ml/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Mean squared error for training and test\n",
    "\n",
    "mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "mse_val = mean_squared_error(y_val, y_val_pred)\n",
    "\n",
    "# r2 error for training and test\n",
    "\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "r2_val = r2_score(y_val, y_val_pred)\n",
    "\n",
    "# Print the results\n",
    "print(\"Training Mean Squared Error: \", mse_train)\n",
    "print(\"Training R-squared: \", r2_train)\n",
    "print(\"Validation Mean Squared Error: \", mse_val)\n",
    "print(\"Validation R-squared: \", r2_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54aa7795",
   "metadata": {},
   "source": [
    "### Step 5: Visualize Results (1 mark)\n",
    "1. Create a pandas DataFrame `results` with columns: Training accuracy and Validation accuracy, and index: MSE and R2 score\n",
    "2. Add the accuracy results to the `results` DataFrame\n",
    "3. Print `results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "88d223f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        MSE        R2\n",
      "0  0.103345  0.564127\n",
      "1  0.114769  0.530041\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "\n",
    "# STEP 5\n",
    "\n",
    "results_data = {\n",
    "    \"MSE\": [mse_train, mse_val],\n",
    "    \"R2\": [r2_train, r2_val]\n",
    "}\n",
    "\n",
    "results = pd.DataFrame(results_data)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a42bda",
   "metadata": {},
   "source": [
    "### Questions (2 marks)\n",
    "1. Did using a linear model produce good results for this dataset? Why or why not?\n",
    "\n",
    "MSE: The Mean Squared Error measures the average squared difference between the actual target values and the predicted values. In this case, a lower MSE indicates better model performance. The training MSE is 0.103345, and the validation MSE is 0.114769. The training MSE is slightly lower, suggesting that the model fits the training data better. However, the difference between the training and validation MSE is relatively small, which is a positive sign.\n",
    "\n",
    "R2: The R-squared (R2) score measures the proportion of the variance in the target variable that is predictable from the independent variables. A higher R2 score indicates better model fit. The training R2 is 0.564127, and the validation R2 is 0.530041. These values suggest that the model explains a reasonable portion of the variance in the data, with the training set performing slightly better.\n",
    "\n",
    "In summary, the linear regression model seems to perform reasonably well on this dataset. The training and validation metrics are close, indicating that the model generalizes well to unseen data. However, the goodness of fit can be further assessed by comparing these metrics to other models and conducting cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca0ff2f",
   "metadata": {},
   "source": [
    "### Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "1. Where did you source your code?\n",
    "Code was sourced primarily from lab exercises, as the example shown was similar. Generative AI tool ChatGPT was utilized in some troubleshooting of the code.\n",
    "1. In what order did you complete the steps?\n",
    "a. Step 1: Loading the data using the Yellowbrick library.\n",
    "b. Step 2: Checking for missing values in the dataset.\n",
    "c. Step 3: Implementing a machine learning model (either LinearRegression or LogisticRegression based on your request) using scikit-learn.\n",
    "d. Step 4: Evaluating the model's performance by calculating MSE and R-squared for both training and validation sets.\n",
    "e. Step 5: Creating a DataFrame to present the results.\n",
    "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "ChatGPT assisted with troubleshooting of my code. Specifically, I kept encountering the following error in most of my code:\n",
    "/Users/satchy/anaconda3/envs/ensf-ml/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
    "ChatGPT explained that this code could be ignored but also suggested using .values at the end of my variables when importing them into my datasets to avoid this warning message which I ended up implemented throughout the code.\n",
    "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?\n",
    "I found it difficult to know exactly what needed to be done in each step but following lab examples helped trememnedously. It would have been very difficult to complete this assignment otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72ac3eb",
   "metadata": {},
   "source": [
    "## Part 3: Observations/Interpretation (3 marks)\n",
    "\n",
    "Describe any pattern you see in the results. Relate your findings to what we discussed during lectures. Include data to justify your findings.\n",
    "\n",
    "\n",
    "*ADD YOUR FINDINGS HERE*\n",
    "\n",
    "MSE Comparison:\n",
    "The training MSE (Mean Squared Error) is 0.103345.\n",
    "The validation MSE is 0.114769.\n",
    "Interpretation: The MSE measures the average squared difference between the actual and predicted values. A lower MSE indicates a better model fit. In this case, the training MSE is slightly lower than the validation MSE, which is a common pattern. The model typically fits the training data better than the validation data.\n",
    "\n",
    "R2 Comparison:\n",
    "The training R2 (R-squared) is 0.564127.\n",
    "The validation R2 is 0.530041.\n",
    "Interpretation: R2 measures the proportion of the variance in the target variable that is predictable from the independent variables. A higher R2 indicates a better model fit. Both the training and validation R2 values are relatively close to each other, suggesting that the model generalizes reasonably well to new, unseen data.\n",
    "\n",
    "Training vs. Validation:\n",
    "The training MSE and R2 are slightly better than the validation MSE and R2.\n",
    "Interpretation: This is a typical pattern. The training set is the data the model was trained on, so it tends to fit this data better. The validation set is used to assess how well the model generalizes to new, unseen data. The relatively small difference between the training and validation metrics suggests that the model's performance is consistent and does not show signs of severe overfitting.\n",
    "\n",
    "Overall, these results align with what is often discussed in machine learning lectures:\n",
    "Generalization: The model appears to generalize well, as indicated by the similarity in performance between the training and validation sets. This suggests that the model is not overfitting the training data.\n",
    "Model Fit: The model shows reasonable performance in explaining the variance in the target variable. However, the results may vary depending on the specific dataset and problem. Further analysis, such as comparing this linear model with more complex models or conducting cross-validation, would provide a more comprehensive assessment of the model's fit to the data.\n",
    "Model Evaluation: It's crucial to evaluate regression models using appropriate metrics like MSE and R2. These metrics provide insights into how well the model predicts the target variable.\n",
    "\n",
    "In summary, the observed pattern in the results aligns with the principles discussed during machine learning lectures, emphasizing the importance of model evaluation, generalization, and the choice of appropriate metrics for assessing model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b84eed",
   "metadata": {},
   "source": [
    "## Part 4: Reflection (2 marks)\n",
    "Include a sentence or two about:\n",
    "- what you liked or disliked,\n",
    "- found interesting, confusing, challangeing, motivating\n",
    "while working on this assignment.\n",
    "\n",
    "\n",
    "*ADD YOUR THOUGHTS HERE*\n",
    "\n",
    "While assisting with this assignment, I enjoyed the clarity of the tasks and questions provided. It made it straightforward to create the code and provide explanations. However, I also noticed that some of the provided code contained inconsistencies, such as references to \"Logistic Regression\" when working with a linear regression model, which could potentially lead to confusion. Ensuring that the code and terminology are consistent with the task description is essential for clarity and understanding. Overall, it was motivating to help with this assignment and provide guidance on data processing, modeling, and evaluation, which are fundamental concepts in machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db951b3a",
   "metadata": {},
   "source": [
    "## Part 5: Bonus Question (4 marks)\n",
    "\n",
    "Repeat Part 2 with Ridge and Lasso regression to see if you can improve the accuracy results. Which method and what value of alpha gave you the best R^2 score? Is this score \"good enough\"? Explain why or why not.\n",
    "\n",
    "**Remember**: Only test values of alpha from 0.001 to 100 along the logorithmic scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "47623d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Ridge R-squared: 0.5308491236372105\n",
      "Best Ridge Alpha: 55.90810182512222\n",
      "Best Lasso R-squared: 0.5287688594784852\n",
      "Best Lasso Alpha: 0.001\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Define a range of alpha values on a logarithmic scale\n",
    "alphas = np.logspace(-3, 2, 100)\n",
    "\n",
    "best_r2 = -np.inf  # Initialize to negative infinity\n",
    "best_alpha = None\n",
    "best_model = None\n",
    "\n",
    "# Loop through different alpha values\n",
    "for alpha in alphas:\n",
    "    # Create and fit a Ridge model\n",
    "    ridge_model = Ridge(alpha=alpha)\n",
    "    ridge_model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the validation set\n",
    "    y_val_pred_ridge = ridge_model.predict(X_val)\n",
    "\n",
    "    # Calculate R-squared for Ridge\n",
    "    r2_val_ridge = r2_score(y_val, y_val_pred_ridge)\n",
    "\n",
    "    if r2_val_ridge > best_r2:\n",
    "        best_r2 = r2_val_ridge\n",
    "        best_alpha = alpha\n",
    "        best_model = ridge_model\n",
    "\n",
    "print(\"Best Ridge R-squared:\", best_r2)\n",
    "print(\"Best Ridge Alpha:\", best_alpha)\n",
    "\n",
    "# Repeat the process for Lasso\n",
    "best_r2 = -np.inf\n",
    "best_alpha = None\n",
    "best_model = None\n",
    "\n",
    "for alpha in alphas:\n",
    "    # Create and fit a Lasso model\n",
    "    lasso_model = Lasso(alpha=alpha)\n",
    "    lasso_model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the validation set\n",
    "    y_val_pred_lasso = lasso_model.predict(X_val)\n",
    "\n",
    "    # Calculate R-squared for Lasso\n",
    "    r2_val_lasso = r2_score(y_val, y_val_pred_lasso)\n",
    "\n",
    "    if r2_val_lasso > best_r2:\n",
    "        best_r2 = r2_val_lasso\n",
    "        best_alpha = alpha\n",
    "        best_model = lasso_model\n",
    "\n",
    "print(\"Best Lasso R-squared:\", best_r2)\n",
    "print(\"Best Lasso Alpha:\", best_alpha)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b606236",
   "metadata": {},
   "source": [
    "*ANSWER HERE*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
